---
title: "Project 1 -Forecasting "
author: "Rachel Greenlee"
date: "3/18/2022"
output:
  
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE,
                      out.width = "100%")

library("dplyr")
library("fpp3")
library("tsibble")
library('tidyr')
library('fable')
library("lubridate")
library("plotly")
library("readr")
library("skimr")
library("stringr")
library("forecast")
library("imputeTS")
library("flextable")
library("ggpubr")

##making theme
mpeach <- "#fbaa82"
mteal <- "#73a2ac"
mdarkteal <- "#0b5d69"
mgray <- "#4c4c4c"

# set plot theme for assignment
my_plot_theme <- list(
  theme_classic() +
  theme(plot.background = element_rect(fill = "#F3F2E8"),
        panel.background = element_rect(fill = "#F3F2E8"),
        panel.grid.major.x = element_line(color = "white"),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
        text = element_text(size = 12)))
```



**Midterm project for DATA624 course in CUNY's MSDS program**


# Part A - ATM Forecast

#### In part A, I want you to forecast how much cash is taken out of 4 different ATM machines for May 2010.  The data is given in a single file.  The variable ‘Cash’ is provided in hundreds of dollars, other than that it is straight forward.   I am being somewhat ambiguous on purpose to make this have a little more business feeling.  Explain and demonstrate your process, techniques used and not used, and your actual forecast.  I am giving you data via an excel file, please provide your written report on your findings, visuals, discussion and your R code via an RPubs link along with the actual.rmd file  Also please submit the forecast which you will put in an Excel readable file.


## #1 Data preparation  


First I load in the provided data and take a look at the first few rows. I see that we have a date column, which I recreate in a standardized format that will be recognized as a date by a tsibble later.

```{r}
atm_raw <- read_csv("atm_raw.csv")
head(atm_raw)

#start cleaned dataset
atm <- atm_raw

#set to date
atm$DATE = as.Date(atm$DATE, format='%m/%d/%Y 12:00:00 AM', origin = '1990-01-01')

```

A first glance at the data I see there is some work to do. I see 14 cases having no value for `ATM`, these can be removed as I see these are associated with May dates (which we'll be predicting later) and no `Cash` value is present. I also see that ATM1 and atm4 have a couple missing values. ATM3 appears to have a strange distribution with nearly all the percentiles having a value of zero.

```{r}
atm %>%
  group_by(ATM) %>%
  skim()
```


I drop the cases where the `ATM` variable is NA, as these also had empty `Cash` values. This will not result in an issue of continuity in our time series data, as these are the value at the end of the dates that will be predicted later.

```{r}
#remove cases with ATM=NA
atm <- atm %>%
  drop_na(ATM)
```


Let's take a closer look at ATM3 cash values. It appears there are 362 days with a value of zero `Cash` withdrawn, and 1 day each for 8,200, 8,500, and 9,600. 
```{r}
atm %>%
  filter(ATM == "ATM3") %>%
  group_by(Cash) %>%
  summarize(n = n())
```

Looking at the only cases where ATM3 has greater than zero for the `Cash` variable, I see that these are the last 3 days of the time series data. This suggests the machine was previously broken or was only installed on 4/28/2010, explaining the lack of data for the majority of this time series dataset. As a prediction based off only 3 data points would not be wise, I'll disregard ATM3 moving forward and relay to my boss that we'll need more data before we can forecast for ATM3.
```{r}
atm %>%
  filter(ATM == "ATM3",
         Cash > 0)
```



Finally, I'll set each ATM to it's own dataframe.
```{r}
atm$Cash <- as.numeric(unlist(atm$Cash))


atm1 <- atm %>%
  filter(ATM == "ATM1")

atm2 <- atm %>%
  filter(ATM == "ATM2")

atm4 <- atm %>%
  filter(ATM == "ATM4")

```


### Imputation {.tabset}

Next I'll see how the `imputeTS` package would impute the handful of NA values using the interpolation method of imputation. This is a fairly basic method, but it essentially splits fills in the missing value halfway between the previous and next value in the time series. If I had more missing values, or missing values bunched together, I might consider more robust methods but this will suffice for this situation.

#### ATM1

The red values it's suggesting appear reasonable to my eye, and considering it's a very small proportion of the full dataset (0.8%) this is acceptable to use in order to achieve a complete time series dataset.

```{r}
#store imputation
imputation <- na_interpolation(atm1$Cash)

#plot imputation
ggplot_na_imputations(atm1$Cash, imputation) +
  my_plot_theme

#incorporation imputation
atm1$Cash <- na_interpolation(atm1$Cash)

```




#### ATM2

And again for ATM2 we see reasonable values for the interpolation imputation method.

```{r}
#store imputation
imputation <- na_interpolation(atm2$Cash)

#plot imputation
ggplot_na_imputations(atm2$Cash, imputation) +
  my_plot_theme

#incorporation imputation
atm2$Cash <- imputation
```



## #2 Visualize the data

I plot each ATM as a boxplot below to get a better look at the distributions. ATM1 and atm2 seem the most normally distributed, though there are quite a few outliers on the low end in ATM1. Further, ATM1 has the highest median of the three ATMs which might suggest it's in a wealthier area. Finally, atm2 has a very small range of values, however, taking note of the scale this is an optics issue caused by that extreme outlier well over $900,000. I've stumbled on another data cleaning issue.

```{r}
atm1_box <- ggplot(atm1, aes(x = Cash)) +
                     geom_boxplot()

atm2_box <- ggplot(atm2, aes(x = Cash)) +
                     geom_boxplot()

atm4_box <- ggplot(atm4, aes(x = Cash)) +
                     geom_boxplot()
  
  
ggarrange(atm1_box, atm2_box, atm4_box,
  labels = c("ATM1", "ATM2", "ATM4"),
  ncol = 1) +
  my_plot_theme
```



### Check for outliers {.tabset}

A nice function from the `forecast` package allows it to see the number of cases that are outliers. The function `tsoutliers()` has thorough documentation, but essentially is better suited to detected outliers of a *time series* more than standard data as it decomposes the series and considers seasonality and trend before determining outliers. I'll now convert each ATM dataframe into a tsibble object so I can use the forecast package's functionality moving forward.


#### ATM1

ATM1 has over 30 outliers, which is consistent with the boxplot above. We also see the recommended replacement values. While these are statistically identified as outliers, there are so many it would be unwise to remove and/or replace these values. It's good to know these exist, but I won't fiddle with them.

```{r}
#make tsibble version that works with tsoutliers
atm1 <- as.data.frame(atm1)
atm1_ts <- atm1 %>%
  select(-DATE) %>%
  mutate(Date = as.Date("2009-05-01") + 0:364) %>%
  as_tsibble(index = Date, key = Cash)

#check for outliers
tsoutliers(atm1_ts$Cash)
```
```{r}
#make tsibble version that will work with autoplot later
atm1_ts <- as.data.frame(atm1_ts)

atm1_ts <- tsibble(
  Date = as.Date("2009-05-01") + 0:364,
  Cash = atm1$Cash)
```


#### ATM2

ATM2 has far fewer outliers, only 5. These didn't show up on the boxplot above, but that is likely because the `tsoutliers()` function found outliers with regard to the seasonality and/or trend, whereas the box-plot is just looking at raw numbers and a raw distribution. While these values are certainly high, I don't believe they are in error so I won't remove/replace these either.

```{r}
#make tsibble version that works with tsoutliers
atm2 <- as.data.frame(atm2)
atm2_ts <- atm2 %>%
  select(-DATE) %>%
  mutate(Date = as.Date("2009-05-01") + 0:364) %>%
  as_tsibble(index = Date, key = Cash)

#check for outliers
tsoutliers(atm2_ts$Cash)
```

```{r}
#make tsibble version that will work with autoplot later
atm2_ts <- as.data.frame(atm2_ts)

atm2_ts <- tsibble(
  Date = as.Date("2009-05-01") + 0:364,
  Cash = atm2$Cash)
```



#### ATM4

ATM4 has the highest number of outliers we've seen, and it's unclear from this readout which is the extreme value seen in the boxplot.

```{r}
#make tsibble version that works with tsoutliers
atm4 <- as.data.frame(atm4)
atm4_ts <- atm4 %>%
  select(-DATE) %>%
  mutate(Date = as.Date("2009-05-01") + 0:364) %>%
  as_tsibble(index = Date, key = Cash)

#check for outliers
tsoutliers(atm4_ts$Cash)
```




I filter by values where Cash is greater than 10,000 and find this on the date 02-09-2010. A this value is so extreme, 27 times greater than the median, I'll replace it with the median. I'm assuming this must be in error, or maybe a day the ATM was robbed! If I were in a setting with coworkers I'd certainly ask my team if anyone knew what may have happened that day, I'd imagine a single ATM doesn't even contain that much cash at any given time - if I was in the industry I'd hopefully know this max amount. 

```{r}


#make tsibble version that will work with autoplot later
atm4_ts <- as.data.frame(atm4_ts)

atm4_ts <- tsibble(
  Date = as.Date("2009-05-01") + 0:364,
  Cash = atm4$Cash)


#finder outlier
atm4_ts %>%
  filter(Cash > 10000)

#set outlier to NA for now
atm4_ts$Cash[atm4_ts$Cash=="10920"]<-NA

#calculate median w/o extreme outlier
atm4_median <- median(atm4_ts$Cash, na.rm = TRUE)

#put median in for outlier/NA value
atm4_ts$Cash[atm4_ts$Cash==NA]<-atm4_median






```



I revisit the boxplot and see a few outliers on the high end, but nothing as extreme as I saw earlier.

```{r}
ggplot(atm4_ts, aes(x = Cash)) +
  geom_boxplot() +
  my_plot_theme
```



### Consider Transformations {.tabset}

Now I can proceed looking for trend, seasonality, and/or the need for Box-Cox transformation on each of the ATMs' time series data.  

#### ATM1

A plot of the cleaned ATM1 data shows some changes in variance and certainly seasonality.


```{r}
autoplot(atm1_ts, Cash)

```
In order to better regulate the variation in the plot above, I’ll perform a Box-Cox transformation. In this case lambda = 0.35. The plot still has plenty of seasonality, but the variance is more stable.

```{r}
#calculate lambda
lambda1 <- atm1_ts %>%
  features(Cash, features = guerrero) %>%
  pull(lambda_guerrero)


#plug into autoplot
ggplotly(atm1_ts %>%
    autoplot(box_cox(Cash, lambda1)) +
     labs(y = "",
       title = "Transformed ATM1 Cash with lambda = 0.35"))

#apply transformation
atm1_tf <-
    atm1_ts %>% 
    mutate(Cash = box_cox(Cash, lambda1))
```


Now I’ll check the residuals. I clearly see lags on day 7, 14, and 21 on the ACF plot, which makes sense as it’s likely seasonal around the days of the week. I’ll definitely need a model that incorporates this seasonality.


```{r}
#check residuals
ggtsdisplay(atm1_tf$Cash, main="ATM1 Residuals after Box-Cox")
```


#### ATM2

A plot of the cleaned atm4 data shows some changes in variance and certainly seasonality - similar to ATM1.


```{r}
autoplot(atm2_ts, Cash)
```

In order to better regulate the variation in the plot above, I’ll perform a Box-Cox transformation. In this case lambda = 0.68. The plot still has plenty of seasonality, but the variance is more stable.

```{r}
#calculate lambda
lambda2 <- atm2_ts %>%
  features(Cash, features = guerrero) %>%
  pull(lambda_guerrero)


#plug into autoplot
ggplotly(atm2_ts %>%
    autoplot(box_cox(Cash, lambda2)) +
     labs(y = "",
       title = "Transformed atm4 Cash with lambda = 0.68"))
#apply transformation
atm2_tf <-
    atm2_ts %>% 
    mutate(Cash = box_cox(Cash, lambda2))

```


Now I’ll check the residuals. Again I see lags at multiples of 7, but also more prominent ones at 2, 5, and 9 that will have to be checked on.

```{r}
#check residuals
ggtsdisplay(atm2_tf$Cash, main="atm4 Residuals after Box-Cox")
```



#### ATM4

A plot of the cleaned ATM4 data shows seasonality with some of the most variance we’ve seen so far.

```{r}
autoplot(atm4_ts, Cash)

```

In order to better regulate the variation in the plot above, I’ll perform a Box-Cox transformation. In this case lambda = 0.40. The plot still has plenty of seasonality, but the variance is more stable.

```{r}
#calculate lambda
lambda4 <- atm4_ts %>%
  features(Cash, features = guerrero) %>%
  pull(lambda_guerrero)


#plug into autoplot
ggplotly(atm4_ts %>%
    autoplot(box_cox(Cash, lambda4)) +
     labs(y = "",
       title = "Transformed ATM4 Cash with lambda = 0.40"))

#apply transformation
atm4_tf <-
    atm4_ts %>% 
    mutate(Cash = box_cox(Cash, lambda4))

```

Now I’ll check the residuals. I see primarily lags at multiples of 7, not at the other intervals seen in atm4.


```{r}
#check residuals
ggtsdisplay(atm4_tf$Cash, main="ATM4 Residuals after Box-Cox")
```






## #3 Choose your model

It’s clear all of our ATMs will need modeling that incorporated their seasonality around the weekdays. I’ll try the simplest option, the Seasonal Naive Method first on each - this can also work as a baseline model to compare the others to. Next, I'll try an ETS model, as they can be used on non-stationary data and it operates based on weighted averages, favoring the more recent data points. Holt-Winter's Seasonal smoothing will be most appropriate as it considers both trend and seasonality. Finally, I'll see what the auto-selected ARIMA model can come up with.  

After evaluating the performance of each of these, for each ATM, I'll choose my model before produce forecasts.





## #4 Train the model  {.tabset}

### ATM1

```{r}
#shave off April 2010 to make our training dataset
atm1_train <- atm1_ts %>%
  filter(Date < as.Date('2010-04-01'))
```


#### Seasonal Naive Model

To start this Seasonal Naive Model (in blue) appears to not capture the range of April 2010, and further is sometimes high when the seasonality is low in the real data. This doesn't appear to be a great fit from just looking at the plot, but it isn't terrible.


```{r}
#save the model
atm1_fit_snaive <- atm1_train %>%
  model(SNAIVE(Cash))

#create forecast
atm1_forecast_snaive <- atm1_fit_snaive %>%
  forecast(new_data = anti_join(atm1_ts, atm1_train))

#plot forecast against real April data
atm1_forecast_snaive %>% autoplot(atm1_ts) +
  labs(title = "ATM1 - Seasonal Naive Model",
       y = "USD in Hundreds")
```

After saving the model fit and checking the residuals, I see the above model is not capturing the 7-day lag (days of the week).

```{r}

#check residuals
atm1_fit_snaive %>% 
  gg_tsresiduals()

```

Finally, in looking at the accuracy we see an RMSE of 16.08 and a MAE of 13.53. As our book authors recommend using RMSE and/or MAE to compare forecast methods. I'll compare future ATM1 scores to this base Seasonal Naive Model - and remember that the lower these values the better the fit.

```{r}
#calculate accuracy
atm1_forecast_snaive %>% 
  accuracy(atm1_ts)

#store in df
models_atm1 <- data.frame(model = "seasonal naive",
                                RMSE = "16.03746",
                                MAE = "13.53333")
```



#### ETS Model

Next I check the Winter-Holt Seasonal Exponential Smoothing Model. I chose the multiplicative method for this model as the seasonal variations are changing proportional to the point in time for this time series, the seasonality isn't completely steady. As this data isn't show any major trends, I don't considered the damped method.  

Visually, this appears to fit a lot better, particularly the pattern of when to go up or down, though the magnitude at times is a little off.

```{r}
#save the model
atm1_fit_etsAAM <- atm1_train %>%
  model(ETS(Cash ~ error("A") +
                       trend("A") + season("M")))

#create forecast
atm1_forecast_etsAAM <- atm1_fit_etsAAM %>%
  forecast(new_data = anti_join(atm1_ts, atm1_train))

#plot forecast against real April data
atm1_forecast_etsAAM %>% autoplot(atm1_ts) +
  labs(title = "ATM1 - ETS AAM",
       y = "USD in Hundreds")
```
The residual plots look much better than the Seasonal Naive Model, though we still have two value outside the ideal range on the ACF plot it isn't by too much.


```{r}
#check residuals
atm1_fit_etsAAM %>% 
  gg_tsresiduals()
```
I find the RMSE and MAE of this model and add it to me dataframe comparing all of the models I'm trying for ATM1.


```{r}
#calculate accuracy
atm1_forecast_etsAAM %>% 
  accuracy(atm1_ts)
```



```{r}
#add to model list
models_atm1[nrow(models_atm1) +1,] <- c("winter-holt seasonal, mult", "11.41895", "8.887111")
```

#### ARIMA Model

Last I'll build an ARIMA Model. This model type requires the data be stationary. I look at the plotted data and do not see signs of non-stationarity, there isn't too much up and down of the seasonality across time.

```{r}
atm1_train %>%
  autoplot()
```
Looking at the ACF plot I don't see the characteristic high R1 value associated with non-stationary data, only the significant lags every 7 days.

```{r}
#acf plot only
atm1_train %>%
  ACF(Cash) %>%
  autoplot()

```

Further, the ndiffs() function suggests no difference for this time series. Since our p-value on the KPSS test is greater than 0.05, we conclude the series is stationary and ready for ARIMA modeling.

```{r}
ndiffs(atm1_train$Cash)

unitroot_kpss(atm1_train$Cash)
```

Allowing the algorithm to select the best ARIMA model, it chooses a ARIMA(0,0,1)(0,1,2)[7]. While the AIC is displayed, we don't have these values for the other models so will need to find the RMSE and MAE.


```{r}
#use autoselect ARIMA model
atm1_fit_ARIMA <- atm1_train %>%
  model(ARIMA(Cash))

report(atm1_fit_ARIMA)
```

Below I see the April 2010 forecast from the selected ARIMA model against the actual April 2010 data. This looks to be fit about as well as the ETS model, but it's hard to tell which is better by eye.

```{r}
#create forecast
atm1_forecast_ARIMA <- atm1_fit_ARIMA %>%
  forecast(new_data = anti_join(atm1_ts, atm1_train))

#plot forecast against real April data
atm1_forecast_ARIMA %>% autoplot(atm1_ts) +
  labs(title = "ATM1 - ARIMA",
       y = "USD in Hundreds")
```
Checking the residuals plot it appears there is no pattern and it's simply white noise, which is good.

```{r}
atm1_fit_ARIMA %>%
  gg_tsresiduals()
```


Finding the accuracy of the forecast, I see an RMSE of 12.38991 and a MAE of 9.706059.

```{r}
#calculate accuracy
atm1_forecast_ARIMA %>% 
  accuracy(atm1_ts)
```


```{r}
#add to model list
models_atm1[nrow(models_atm1) +1,] <- c("arima", "12.38991", "9.706059")
```


### ATM2


```{r}
#shave off April 2010 to make our training dataset
atm2_train <- atm2_ts %>%
  filter(Date < as.Date('2010-04-01'))
```


#### Seasonal Naive Model

Again I'll start with a Seasonal Naive Model. This model visually appears to have predicted a few more oscilations than we see in the actual April 2010 data, so this again may not be the best fit.


```{r}
#save the model
atm2_fit_snaive <- atm2_train %>%
  model(SNAIVE(Cash))

#create forecast
atm2_forecast_snaive <- atm2_fit_snaive %>%
  forecast(new_data = anti_join(atm2_ts, atm2_train))

#plot forecast against real April data
atm2_forecast_snaive %>% autoplot(atm2_ts) +
  labs(title = "ATM2 - Seasonal Naive Model",
       y = "USD in Hundreds")
```
The residuals show a huge lag at 7 on the ACF plot, a further indication we are missing that critical seasonality for atm4.

```{r}
#check residuals
atm2_fit_snaive %>% 
  gg_tsresiduals()
```

I'll calculate and store the accuracy of this model, but I find it likely the ETS and ARIMA will outperform it easily.

```{r}
#calculate accuracy
atm2_forecast_snaive %>% 
  accuracy(atm2_ts)

#store in df
models_atm2 <- data.frame(model = "seasonal naive",
                                RMSE = "26.16231",
                                MAE = "17.8")

```


#### ETS Model

Next I'll try the Winter-Holt Seasonal Exponential Smoothing Model. I'll choose the additive method for ATM2 as the seasonal variations do not seem to change much dependent on the point in the time series. From looking at the plot, the pattern appears correct but it seems to be dipping a bit early each season than the actual April 2010 data. This might not be the best fit for atm4 either.

```{r}
#save the model
atm2_fit_etsAAA <- atm2_train %>%
  model(ETS(Cash ~ error("A") +
                       trend("A") + season("A")))

#create forecast
atm2_forecast_etsAAA <- atm2_fit_etsAAA %>%
  forecast(new_data = anti_join(atm2_ts, atm2_train))

#plot forecast against real April data
atm2_forecast_etsAAA %>% autoplot(atm2_ts) +
  labs(title = "ATM2 - ETS AAA",
       y = "USD in Hundreds")
```

This residual plot certainly looks better than the Seasonal Naive, but we still have lags at a few values a bit further out of the bounds than is ideal.

```{r}
#check residuals
atm2_fit_etsAAA %>% 
  gg_tsresiduals()
```

The RMSE and MAE values are lower than the Seasonal Naive, but I'm hopeful the ARIMA will do even better in this case.

```{r}
#calculate accuracy
atm2_forecast_etsAAA %>% 
  accuracy(atm2_ts)
```



```{r}
#add to model list
models_atm2[nrow(models_atm1) +1,] <- c("winter-holt seasonal, add", "20.62269", "15.24716")
```



#### ARIMA Model

Last we'll try an ARIMA Model, but first we check if our data is stationary. It looks even more stationary than ATM1 did, and that one didn't require and differencing.

```{r}
atm2_train %>%
  autoplot()
```
My eyes must be off, it appears 1 difference is appropriate.

```{r}
ndiffs(atm2_train$Cash)

```

I make a new tsibble of the training data and apply the differencing. I check to see if any furthering differencing is needed, and receive a 0.

```{r}

atm2_train_diff <- atm2_train %>%
  mutate(Cash = difference(Cash))

#check again
ndiffs(atm2_train_diff$Cash)
```

And with a 0.1 value on the KPSS test the data is now considered stationary, as we accept the null hypothesis of this test. 


```{r}
unitroot_kpss(atm2_train_diff$Cash)
```


For the differenced AT<2 data it selects an ARIMA(3,0,0)(2,1,0)[7]. 

```{r}
#use autoselect ARIMA model
atm2_fit_ARIMA <- atm2_train_diff %>%
  model(ARIMA(Cash))

report(atm2_fit_ARIMA)
```
A visual inspection of the forecast shows the predicted values going quite a bit lower than the rest of the series, not a very good fit.


```{r}
#create forecast
atm2_forecast_ARIMA <- atm2_fit_ARIMA %>%
  forecast(h = 30)

#plot forecast against real April data
atm2_forecast_ARIMA %>% autoplot(atm2_ts) +
  labs(title = "ATM2 - ARIMA",
       y = "USD in Hundreds")
```


Our residuals plot shows 6 values out of the ideal bounds on the ACF, showing the residuals are more than white noise.


```{r}
atm2_fit_ARIMA %>%
  gg_tsresiduals()
```




Again, we'll need to find compatible measures of fit to the Seasonal Naive & ETS Models, so I calculate them and store below.

```{r}
#calculate accuracy
atm2_forecast_ARIMA %>% 
  accuracy(atm2_ts)
```
```{r}
#add to model list
models_atm2[nrow(models_atm2) +1,] <- c("arima", "77.43501", "63.42228")
```





### ATM4

```{r}
#shave off April 2010 to make our training dataset
atm4_train <- atm4_ts %>%
  filter(Date < as.Date('2010-04-01'))


```


#### Seasonal Naive Model

I'll start with a simple Seasonal Naive Model. Visually, these predictions look decent, but about 2/3 the way into April it's quite a bit high compared to the actual data for the month.

```{r}
#save the model
atm4_fit_snaive <- atm4_train %>%
  model(SNAIVE(Cash))

#create forecast
atm4_forecast_snaive <- atm4_fit_snaive %>%
  forecast(new_data = anti_join(atm4_ts, atm4_train))

#plot forecast against real April data
atm4_forecast_snaive %>% autoplot(atm4_ts) +
  labs(title = "ATM4 - Seasonal Naive Model",
       y = "USD in Hundreds")
```

The residuals once again show that missing lag on 7 days.

```{r}
#check residuals
atm4_fit_snaive %>%
  gg_tsresiduals()
```
I'll calculate and store the accuracy measures.

```{r}
#calculate accuracy
atm4_forecast_snaive %>% 
  accuracy(atm4_ts)
```

```{r}
#store in df
models_atm4 <- data.frame(model = "seasonal naive",
                                RMSE = "301.022",
                                MAE = "227.3667")
```


#### ETS Model

Again I'll try an ETS Winter-Holt Seasonal method, with multiplicative method for the variation in the variation across the time series seen in the autoplot earlier in the Visualize the data section. Unfortunately, after 2 hours of troubleshooting I cannot determine why, regardless of how I reconfigure the tsibble, my ETS models only return null models. This happens when I specify the terms and when I don't. A fraction of the code I attempted below. Possibly this means the model isn't appropriate for this data, but I wasn't able to find confirmation of this in any resources online or in our textbook.

```{r}
# attempt 1 at tsibble
atm4_train_ts <- as.data.frame(atm4_train)
atm4_train_ts <- tsibble(
  Date = as.Date("2009-05-01") + 0:334,
  Cash = atm4_train_ts$Cash)

#attempt 2 at tsibble
atm4_train_df <- as.data.frame(atm4_train)

atm4_train_ts_2 <- atm4_train_df %>%
  select(-Date) %>%
  mutate(Date = as.Date("2009-05-01") + 0:334) %>%
  as_tsibble(index = Date, key = Cash)

#save the model ##just a null models
atm4_fit_etsAAM <- atm4_train_ts %>%
  model(ETS(Cash))

#error can't find variable
# atm4_fit_etsAAM <- atm4_train_ts_2 %>%
#   model(ETS(Cash))



```



#### ARIMA Model

We see a plot and next check for differencing.

```{r}
atm4_train %>%
  autoplot()
```
No differencing is needed.


```{r}
ndiffs(atm4_train$Cash)

```


An ARIMA(0,0,0)(2,0,0)[7] w/ mean is chosen by the auto-selector.

```{r}
#use autoselect ARIMA model
atm4_fit_ARIMA <- atm4_train %>%
  model(ARIMA(Cash))

report(atm4_fit_ARIMA)
```
The forecast below looks like a poor fit, not capturing the seasonality.

```{r}
#create forecast
atm4_forecast_ARIMA <- atm4_fit_ARIMA %>%
  forecast(h = 30)

#plot forecast against real April data
atm4_forecast_ARIMA %>% autoplot(atm4_ts) +
  labs(title = "ATM4 - ARIMA",
       y = "USD in Hundreds")
```
The residual ACF plot is the best I've seen in this assignment so far, but our historgram has a bit of right-skew to the distribution.

```{r}
atm4_fit_ARIMA %>%
  gg_tsresiduals()
```

```{r}
#calculate accuracy
atm4_forecast_ARIMA %>% 
  accuracy(atm4_ts)
```



```{r}
#add to model list
models_atm4[nrow(models_atm4) +1,] <- c("arima", "273.5695", "234.8024")
```



## #5 Evaluate model performance  {.tabset}

### ATM1

I can reference the dataframe I made storing the RMSE and MAE for each model I tried above. It appears the ETS model, a Winter-Holt Seasonal model of the multiplicative type, has both the lowest RMSE and MAE. I will proceed to forecast using this model.

```{r}
models_atm1
```



### ATM2

Looking at the stored RMSE and MAE accuracy measures for the three models, it appear that again the Winter-Holt Seasonal model, an Exponential Smoothing model with the additive method, performs best. In this case the ARIMA performs so poorly I am curious if I've done something wrong in the model-building, but I can't seem to figure out where that happened. I'll have to move forward again with the ETS model being best for ATM2, as it was for ATM1.

```{r}
models_atm2
```



### ATM4

For the two models I was able to get functioning, we see the RMSE is better on the ARIMA and the MAE is better on the Seasonal Naive. Looking at the forecast plots for each above, and remembering that when all things are nearly equal, simple is always better, I'll choose the Seasonal Naive to forecast for ATM4.


```{r}
models_atm4
```




## #6 Produce forecasts  {.tabset}

### ATM1

Using the entire dataset, I refit the model and forecast values for May 2010. I see in the plot below the forecast, with the confidence intervals getting wider/less sure later in May.

```{r}
#save the model
atm1_fit_etsAAM_final <- atm1_ts %>%
  model(ETS(Cash ~ error("A") +
                       trend("A") + season("M")))

#create forecast
atm1_forecast_etsAAM_final <- atm1_fit_etsAAM_final %>%
  forecast(h = 30)

#plot forecast against real April data
atm1_forecast_etsAAM_final %>% autoplot(atm1_ts) +
  labs(title = "ATM1 - ETS AAM May 2010 Predictions",
       y = "USD in Hundreds")
```

Last, I take the forecasted values and save to an Excel CSV file.

```{r}
#take date & forecast values
forecast_ATM1 <- as.data.frame(atm1_forecast_etsAAM_final) %>%
  select(c(Date, `.mean`))

#rename columns for clarity
colnames(forecast_ATM1) <- c('Date', 'Predicted Cash')

#save to excel csv format
write_excel_csv(forecast_ATM1, "forecast_ATM1.csv")

```




### ATM2

Using the entire dataset, I refit the model and forecast values for May 2010. I see in the plot below the forecast, with the confidence intervals getting wider/less sure later in May.

```{r}
#save the model
atm2_fit_etsAAA_final <- atm2_ts %>%
  model(ETS(Cash ~ error("A") +
                       trend("A") + season("A")))

#create forecast
atm2_forecast_etsAAA_final <- atm2_fit_etsAAA_final %>%
  forecast(h=30)

#plot forecast against real April data
atm2_forecast_etsAAA_final %>% autoplot(atm2_ts) +
  labs(title = "ATM2 - ETS AAA",
       y = "USD in Hundreds")
```

Last, I take the forecasted values and save to an Excel CSV file.

```{r}
#take date & forecast values
forecast_atm2 <- as.data.frame(atm2_forecast_etsAAA_final) %>%
  select(c(Date, `.mean`))

#rename columns for clarity
colnames(forecast_atm2) <- c('Date', 'Predicted Cash')

#save to excel csv format
write_excel_csv(forecast_atm2, "forecast_atm2.csv")

```


### ATM4

I refit the model using the full dataset and produce the May 2010 forecasts.

```{r}
#save the model
atm4_fit_snaive_final <- atm4_ts %>%
  model(SNAIVE(Cash))

#create forecast
atm4_forecast_snaive_final <- atm4_fit_snaive_final %>%
  forecast(h = 30)

#plot forecast against real April data
atm4_forecast_snaive_final %>% autoplot(atm4_ts) +
  labs(title = "ATM4 - Seasonal Naive Model",
       y = "USD in Hundreds")
```
Last, I save the forecasted values to an Excel CSV file.

```{r}
#take date & forecast values
forecast_atm4 <- as.data.frame(atm4_forecast_snaive_final) %>%
  select(c(Date, `.mean`))

#rename columns for clarity
colnames(forecast_atm4) <- c('Date', 'Predicted Cash')

#save to excel csv format
write_excel_csv(forecast_atm4, "forecast_ATM4.csv")

```

# Part B - Power Usage Forecast

Part B consists of a simple dataset of residential power usage for January 1998 until December 2013.  Your assignment is to model these data and a monthly forecast for 2014.  The data is given in a single file.  The variable ‘KWH’ is power consumption in Kilowatt hours, the rest is straight forward.    Add this to your existing files above. 


## #1 Data preparation

Look at a summary of the data we see there are 3 variables, `CaseSequence` showing the order of the `YYYY-MMM` variable, and finally the `KWH` variable showing the power usage. `KWH` has one missing value we will want to impute before we proceed.

```{r}
power_raw <- read_csv("power_raw.csv")

skim(power_raw)

```

### Imputation

Below I use the `imputeTS` package to generate a value for the identified NA value, using the linear (default) method. The point seems reasonable and to take into account the seasonality we see in the data so I incorporate it.

```{r}
#store imputation
imputation <- na_interpolation(power_raw$KWH)
#plot imputation
ggplot_na_imputations(power_raw$KWH, imputation) +
  my_plot_theme

#incorporate imputation
power <- power_raw
power$KWH <- imputation

```




## #2 Visualize the data

I start by visualizing the data with a boxplot and see there is one low outlier we may want to check on.


```{r}
power %>%
  subset(select = KWH) %>%
  ggplot(aes(KWH)) +
  geom_boxplot() +
  labs(title = "Checking KWH Distribution") +
  my_plot_theme
```

### Check for Outliers

The outlier shown above on the box plot appears to be from July 2010 and is not marked as an extreme value. In a real-world setting I would be curious to know from coworkers if there was a large outage for part of this month that decreased consumption, or possible a measurement error.

```{r}
power %>%
  filter(KWH < 1000000)
```


Since this outlier is only 12% of the median for this time series, I will impute it with the median as I presume this value must be in error - without being able to check with real-life coworkers.

```{r}
#set outlier to NA for now
power$KWH[power$KWH=="770523"]<-NA

#calculate median w/o extreme outlier
power_median <- median(power$KWH, na.rm = TRUE)

#put median in for outlier/NA value
power$KWH[power$KWH==NA]<-power_median


```

Checking the boxplot again the distribution appears much more normal with still a fair amount of variation.



```{r}
ggplot(power, aes(x = KWH)) +
  geom_boxplot() +
  my_plot_theme
```




### Consider Transformations

After transforming out data to a tsibble, a quick check of the line graph shows we’ll want to do a Box-Cox transformation to address the variance over time.

```{r}
#transfer to tsibble
power_ts <- tsibble(
  Month = yearmonth(power$`YYYY-MMM`),
  KWH = power$KWH)


autoplot(power_ts)

```

The Box-Cox results in a lambda of -0.23 and we can see the transformation addresses the variance seen above.



```{r}
#calculate lambda
lambda <- power_ts %>%
  features(KWH, features = guerrero) %>%
  pull(lambda_guerrero)


#plug into autoplot
ggplotly(power_ts %>%
    autoplot(box_cox(KWH, lambda)) +
     labs(y = "",
       title = "Transformed KWH with lambda = -0.23"))

#apply transformation
power_tf <-
    power_ts %>% 
    mutate(KWH = box_cox(KWH, lambda))
```


The residuals show some seasonality, possibly lower usage in month 4 (April) and high usage in in month 7 (July). This would make sense if it’s data from a region in the USA.

```{r}
#check residuals
ggtsdisplay(power_tf$KWH, main="KWH Residuals after Box-Cox")
```




## #3 Choose your model

This data appears similar to most of the ATM data I worked with, except it has a bit more trend, going up near the end fo the time serires data. I'll try some of the same models again.



## #4 Train the model


```{r}

#shave off2013 to make training
power_train <-  head(power_tf, -12)
```


#### Seasonal Naive Model

The Seasonal Naive Model looks pretty good, though it seems to have a bit more range in the seasonality than the actual data for 2013.

```{r}
#save the model
power_fit_snaive <- power_train %>%
  model(SNAIVE(KWH))

#create forecast
power_forecast_snaive <- power_fit_snaive %>%
  forecast(new_data = anti_join(power_tf, power_train))

#plot forecast against real April data
power_forecast_snaive %>% autoplot(power_tf) +
  labs(title = "KWH Predictions - Seasonal Naive Model",
       y = "KWH")
```
However I see the 12 lag is quite significant.

```{r}
#check residuals
power_fit_snaive %>% 
  gg_tsresiduals()
```


```{r}
#calculate accuracy
power_forecast_snaive %>% 
  accuracy(power_tf)

#store in df
models_power <- data.frame(model = "seasonal naive",
                                RMSE = "0.003209111",
                                MAE = "0.001980767")
```



#### ARIMA

Looking at the training data I suspect we might need differencing.

```{r}
power_train %>%
  autoplot()
```

One differencing is needed and done.
```{r}
ndiffs(power_train$KWH)
```

No more differencing is needed.

```{r}
power_train_diff <- power_train %>%
  mutate(KWH = difference(KWH))

#check again
ndiffs(power_train_diff$KWH)

unitroot_kpss(power_train_diff$KWH)
```


The automic model selection from the ARIMA() function returns a ARIMA(4,0,0)(2,1,0)[12].

```{r}


#use autoselect ARIMA model
power_fit_ARIMA <- power_train_diff %>%
  model(ARIMA(KWH))

report(power_fit_ARIMA)
```
Our forecast of 2014 looks pretty dang good compared to all the other models in this project.

```{r}

#create forecast
power_forecast_ARIMA <- power_fit_ARIMA %>%
  forecast(h = 12)


#plot forecast against real April data
power_forecast_ARIMA %>% autoplot(power_train_diff) +
  labs(title = "Power KWH Predictions - ARIMA",
       y = "KWH")
```
The residuals look quite good other than one out of bounds at 5 on the ACF plot. Histogram of residuals looks mostly normally distributed. 

```{r}
power_fit_ARIMA %>%
  gg_tsresiduals()
```

I get extremely low RMSE and MAE values, this is another very well fitting model, though I should be wary of any potential overfitting.

```{r}
#calculate accuracy
power_forecast_ARIMA %>% 
  accuracy(power_tf)
```

```{r}
#add to model list
models_power[nrow(models_power) +1,] <- c("arima", "4.222868", "4.222867")
```




## #5 Evaluate model performance

Both of these models performed extremely well. In this case the Seasonal Naive Model performed best, and has the bonus of being the most simple.


```{r}
models_power
```




## #6 Produce forecasts

Below I retrain the model on the full dataset, before predicted 12 months of data, all of 2014. In the plot I see both the seasonality and a bit of the trend maintained in the 1-year forecast.

```{r}
#save the model
power_fit_snaive_final <- power_tf %>%
  model(SNAIVE(KWH))

#create forecast
power_forecast_snaive_final <- power_fit_snaive_final %>%
  forecast(h=12)

#plot forecast against real April data
power_forecast_snaive_final %>% autoplot(power_tf) +
  labs(title = "KWH Predictions - Seasonal Naive Model",
       y = "KWH")
```



Finally, these forecasted values are saved. NEEEEEED TO FIX DIFFERENCING BACK

```{r}
#take date & forecast values
forecast_power <- as.data.frame(power_forecast_snaive_final) %>%
  select(c(Month, `.mean`))

#rename columns for clarity
colnames(forecast_power) <- c('Month', 'Predicted KWH')

#forecast_power$`Predicted KWH` <- diffinv(forecast_power$`Predicted KWH`)

#save to excel csv format
write_excel_csv(forecast_power, "forecast_power.csv")
```





