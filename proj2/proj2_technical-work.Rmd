---
title: "Project 2"
author: "Douglas Barley, Rachel Greenlee, Sean Connin"
date: "4/28/2022"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE,
                      out.width = "100%")
options(kableExtra.auto_format = FALSE) #necessary for dlookr & skimr to play nice together
library(tidyverse)
library(tidymodels)
library(skimr) #for summary
library(dlookr) #for outlier check
library(flextable)
library(GGally)

mpeach <- "#FBAA82"
mteal <- "#73A2AC"
mdarkteal <- "#OB5D69"
mgray <- "#4C4C4C"

# set plot theme for assignment
my_plot_theme <- list(
  theme_classic() +
  theme(plot.background = element_rect(fill = "#F3F2E8"),
        panel.background = element_rect(fill = "#F3F2E8"),
        panel.grid.major.x = element_line(color = "white"),
        axis.title.y = element_text(face = "bold"),
        axis.title.x = element_text(face = "bold"),
#needed below because had a real hard time with some charts in HTML
#being large but having tiny tiny text
        text = element_text(size = 20)))

```

# Data Prep

Before beginning any moeling work, we'll need to preprocess the data. We've chosen to use the (relevant) recommended prepocessing steps based on the Tidy Models article linked below:
https://recipes.tidymodels.org/articles/Ordering.html


## Import and Explore

Our data has already been split into a training and testing dataset, so these are loaded in easily.

```{r}
train <- read_csv("training_data.csv")
test <- read_csv("testing_data.csv")
```

Exploring the `train` dataset we see there are 2,571 cases and 33 variarbles - all of which are numeric except for one character variable. The character variable is BrandCode that appears to have 4 unique values.  

With regards to missingness, no variable has an alarming amount of missing data - `MFR` has 212 values missing resulting in a complete case percentage of 91.8. As many of the model we'll want to consider won't handle this missing data well, we'll impute these missing values in the next step. We also obverse drastically different scales and ranges of values in the numeric variables, so scaling and centering will be essential for some of the models we'll consider.

```{r}
skim(train)
```






## 1. Impute

To take a closer look at our missingness, we check for any frequent patterns of the missing data. For 100 cases only the `MFR` value is missing, and for 91 cases only the `BrandCode` is missing. These are the top two patterns of missingness, and we feel comfortable that these values are 'missing completely at random' and not a result of an underlying interaction or phenomena. 

```{r}
# plotting patterns in missingness, top 15 patterns
train %>% 
  plot_na_intersect(only_na = TRUE, typographic = FALSE, n_intersacts = 15)
```

First, taking a closer look at `BrandCode`, our only non-numeric variable, we see it consists of the NAs, and then the first 4 letters of the alphabet. We set this variable to a factor for both the training and testing dataset in the recipe below.

```{r}
unique(train$`Brand Code`)
```

Now we can use knn algorithm to leverage the other available data in order to impute a likely BrandCode for the missing values. Since this variable is categorical a mean/median wouldn't apply here, and a mode would miss some of the relationships the knn algorithm will hopefully detect when imputing the missing value.

```{r}
#create recipe
preprocess_recipe <- recipe(PH ~., data = train)

#add knn impute of Brand Code to recipe
preprocess_recipe <- preprocess_recipe %>% 
  #prevent error with Brand Code name
  step_rename(BrandCode=`Brand Code`) %>%
  step_mutate_at(BrandCode, fn = ~as.factor(.)) %>%
  step_impute_knn(BrandCode)
```

For our numeric variables with missing values, we first check the shape of these variables to see if a mean or median imputation might suffice. From the histograms we see a lot of not-normally distributed variables and many with outliers. This rules out a mean imputation being useful, and while a mode could suffice we'll choose to use knn imputation again as with this smaller dataset it won't be computationally challenging.

```{r, fig.height=14}
train %>%
  #histograms only for numeric variables
  subset(select = -`Brand Code`) %>%
  #reshape
  gather() %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  labs(title = "Checking Distribution of Numeric Predictor Variables") +
  my_plot_theme
```
```{r}
#add knn impute of all numeric variables except PH to recipe
preprocess_recipe <- preprocess_recipe %>% 
  step_impute_knn(all_numeric_predictors())
```


Lastly, with regard to missing data, earlier we saw that 4 cases are missing a PH value. As this is our response variable in the training dataset, cases with no response variable are not helpful to build the model. We remove these rows.

```{r}
preprocess_recipe <- preprocess_recipe %>% 
  step_naomit(PH)
```




## 2. Handle factor levels

We already converted `BrandCode` to a factor prior to it's knn imputation, but lets check the numeric variables to see if any lend themselves to factor levels instead of numeric classes. 

From the following output that shows the count of unique values for each variable, we don't see any that appear to be diachotomous or with only a few levels/values. We will not switch any of the numeric variables to factors.

```{r}
var_unique <- lapply(train, unique)
lengths(var_unique)
```





## 3. Individual transformations for skewness and other issues

In reviewing the histograms's above of the numeric variables, we see multiple that are not normally distributed and could likely benefit form Box Cox transformations. A handful appear bimodal, those we will not perform a Box Cox on. The chosen variables for Box Cox are: `Air Pressurer`, `Carb Volume`, `Density`, `Fill Pressure`, `Hyd Pressure4`, `PSC`, and `PSC Fill`. The function returns that no Box-Cox transformation could be evaluated for `Carb Volume` and `PSC Fill` so the remain as they were originally.



```{r}
preprocess_recipe <- preprocess_recipe %>% 
  step_BoxCox(`Air Pressurer`, `Carb Volume`, `Density`, `Fill Pressure`, `Hyd Pressure4`, `PSC`, `PSC Fill`)
  
```


## 4. Normalization steps (center, scale, range, etc)

Tidymodels' recipes have a very easy way to center and scale all (non-response) variables to have a standard deviation of one and a mean of zero.

```{r}
preprocess_recipe <- preprocess_recipe %>% 
  step_normalize(all_numeric_predictors())
```




## 5. Create dummy variables

We will add to the recipe of preprossing to code `BrandCode`'s 4 factor levels as dummy variables, as this will be needed for some models that cannot handle categorical variables.

```{r}
preprocess_recipe <- preprocess_recipe %>% 
  step_dummy(BrandCode)
```





## Check Correlations

As it's hard to visualize all of these variables and their possible correlations, we numerically check for multicollinearity in the training data. We find many variables with greater than (+/-) 0.8. Knowing such, we'll employ the `step_corr` feature of recipes to determine which features should be removed due to the colinearity. 

```{r}
train %>%
    #filter_if(is.numeric)%>%
    correlate() %>%
    filter(coef_corr > .8 | coef_corr < -.8)%>% # set thresholds to limit output 
    arrange(desc(abs(coef_corr))) %>%
    flextable()

#remove features that are redundant due to colinearity
preprocess_recipe <- preprocess_recipe %>%
  step_corr(all_predictors(),threshold = 0.8)
```




## Complete Preprocessing

All of the above coded steps/decisions are executed and we review the new summary of the training dataset. We now have 28 (27 predictor) variables, a result of step_corr() removing variables with high correlations to others. There are no more missing values, and we've transformed the variables we could, and centered and scaled all of the values. 

```{r}
#pull out the now-transformed training dataset
data_juiced <-juice(prep(preprocess_recipe))

#see summary of now preprocessed training dataset
skim(data_juiced)

```


## delete later

Checking variables post preprocessing - we worried about anything here?

```{r, fig.height=14}
data_juiced %>%
  #reshape
  gather() %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  labs(title = "Checking Distribution of Numeric Predictor Variables") +
  my_plot_theme
```


#### sean & douglas, some functions you may want to use depending on model choice

remove variable with near-zero variance -> https://recipes.tidymodels.org/reference/step_nzv.html
create interaction variables -> https://recipes.tidymodels.org/reference/step_interact.html

Full index Sean shared here -> https://recipes.tidymodels.org/reference/index.html





