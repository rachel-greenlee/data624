---
title: "HW7 - Linear Regression"
author: "Rachel Greenlee"
date: "04/01/2022"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache=FALSE,
                      out.width = "100%")

library("mlbench") #for datasetes used below
library("skimr") #for data exploration
library("tidyverse")
library("caret")
library("elasticnet")
library("RANN")
library(AppliedPredictiveModeling)
library("corrplot") #for correlation plot

```



**Exercises from Chapter 6 of textbook Applied Predictive Modeling by Kuhn & Johnson**


# Exercise 6.2

#### Developing a model to predict permeability (see Sect. 1.4) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug:

#### (a) Start R and use these commands to load the data. The matrix fingerprints contains the 1,107 binary molecular predictors for the 165 compounds, while permeability contains permeability response.

```{r}
data(permeability)
fingerprints <- as.data.frame(fingerprints)
```


#### (b) The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package. How many predictors are left for modeling?  

After creating a new dataframe `fingerprints_thinned` that shows the dataframe after nearZeroVar is used, there are 388 predictor variables remaining. 


```{r}
#remove predictors with low frequencies
fingerprints_thinned <- fingerprints[, -nearZeroVar(fingerprints)]

#check new predictor count
ncol(fingerprints_thinned)
```


#### (c) Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?

```{r}

#add permeability/target varaible
dataset <- cbind(data.frame(permeability), fingerprints_thinned)

#split train/test
set.seed(3190)
sample_set <- sample(nrow(dataset), round(nrow(dataset)*0.75), replace = FALSE)
fingerprints_train <- dataset[sample_set, ]
fingerprints_test <- dataset[-sample_set, ]
```

It looks like 7 is the optimal number of latent variables. 

```{r}
# build the PLS model
pls_model <- train(fingerprints_train[,-1],
                   fingerprints_train$permeability,
                   method = "pls", 
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))

#plot RMSE of ncomps
plot(pls_model$results$RMSE)

```

In looking at the full results, I see the low RMSE value of 10.142 on ncomp 7 as identified above, and the associated R-squared value is 0.608.

```{r}
pls_model$results
```



#### (d) Predict the response for the test set. What is the test set estimate of R2? 

Taking a look at the predictions from this model in comparison to the test set, I see an R-squared of 0.355 meaning the model can account for 35.5% of the variance in the data. The RMSE is 14.131.

```{r}
pred <- predict(pls_model, fingerprints_test)

postResample(pred = pred, obs = fingerprints_test$permeability)
```



#### (e) Try building other models discussed in this chapter. Do any have better predictive performance?  

I'll try a PCR model first. This has a worse R-Squared which means this model can only account for 28.5% of the variance in the dataset. The RMSE is lower than the PLS though.

```{r}
pcr_model <- train(fingerprints_train[,-1],
                   fingerprints_train$permeability,
                   method = "pcr", 
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))

pred <- predict(pcr_model, fingerprints_test)

postResample(pred = pred, obs = fingerprints_test$permeability)
```


Finally I try an elastic net model, which has a better than PCR but worse the PLS R-squared of 33.9% of variance in the dataset explained. This has an RMSE between the two previous models as well, at 13.958.

```{r}
enet_model <- train(fingerprints_train[,-1],
                   fingerprints_train$permeability,
                   method = "enet", 
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))

pred <- predict(enet_model, fingerprints_test)

postResample(pred = pred, obs = fingerprints_test$permeability)
```



#### (f) Would you recommend any of your models to replace the permeability laboratory experiment?

This is a little tricky, as while the PLS has the best R-squared (highest) it has the worst RMSE value. The PCR model has the lower RMSE but quite a bad R-squared at 28.2%. I believe I would choose the original model, as the elastic net model has only a slightly lower RMSE but also loses about 1.5% points of R-Squared.





# Exercise 6.3

#### A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors),measurements of the manufacturing process (predictors), and the response of product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand,manufacturing process predictors can be changed in the manufacturing process.Improving product yield by 1% will boost revenue by approximately one hundred thousand dollars per batch:

#### (a) Start R and use these commands to load the data. The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors)for the 176 manufacturing runs. yield contains the percent yield for each run.

```{r}
data(ChemicalManufacturingProcess)
chem <- as.data.frame(ChemicalManufacturingProcess)
```


#### (b) A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

I use the `RANN` library to impute the missing values. When I do a skim of the new dataframe I see no missing values.


```{r}
knn_imp <- preProcess(chem, "knnImpute")
chem_imp <- predict(knn_imp, chem)

skim(chem_imp)
```



#### (c) Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

```{r}
#split train/test
set.seed(3190)
sample_set <- sample(nrow(chem_imp), round(nrow(chem_imp)*0.75), replace = FALSE)
chem_train <- chem_imp[sample_set, ]
chem_test <- chem_imp[-sample_set, ]
```

I'll build a PLS model for this dataset. I see the optimal value of ncomps is 3 in this case.

```{r}
# build the PLS model
pls_model <- train(chem_train[,-1],
                   chem_train$Yield,
                   method = "pls", 
                   tuneLength = 10,
                   trControl = trainControl(method = "cv"))

#plot RMSE of ncomps
plot(pls_model$results$RMSE)
```

Further, at the level of 3 the RMSE is 0.604 and the R-squared states the model can account for 63.46% of the variance in the dataset.

```{r}
pls_model$results

```




#### (d) Predict the response for the test set.What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?

Now considering the test data, the RMSE is 0.73 with a lower R-squared of 48.6%.

```{r}
pred <- predict(pls_model, chem_test)

postResample(pred = pred, obs = chem_test$Yield)
```





#### (e) Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?

I found an interesting function `varImp()` that can be used to show the importance of variables in different model types. With regard to PLS the score is calculated by: "the variable importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the sums of squares across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the sums of squares."

More here: https://topepo.github.io/caret/variable-importance.html

In my case, it appears the top 5 predictors are process, followed by 2 biologic, and then another 3 process - so the process measures are dominating in this model. 

```{r}
varImp(pls_model)
```





#### (f) Explore the relationships between each of the top predictors and the response.How could this information be helpful in improving yield in future runs of the manufacturing process?

Going back to the original dataset and selecting just the top 5 predictors from above, I build a correlation plot. There appear to be strong positive correlations between `Yield` and `ManufacturingProcess32`, and negative correlations between `Yield` and `ManufacturingProcess36`, `ManufacturingProcess13`, and `ManufacturingProcess17`. 

```{r}
chem_top5 <- chem_imp %>%
  select(c(ManufacturingProcess32, ManufacturingProcess09, ManufacturingProcess36, ManufacturingProcess13, ManufacturingProcess17,Yield))

correlation <- cor(chem_top5)


# plot correlations
corrplot.mixed(correlation, tl.col = 'black', tl.pos = 'lt', 
               number.cex= .9, tl.cex = .7)


```


There are many methods for combing variables that could be used to leverage this information, further the entire dataset could be checked for high correlation amid the predictor variables, with redundant variables removed.






















